# -*- coding: utf-8 -*-
"""Linear Regression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jXAmgMKieiWfkRjQBptg77WFyPFtf8lG
"""

import numpy as np

# Hàm chuẩn hóa dữ liệu
def mean_normalization(X):
    N = len(X)
    maxi = np.max(X, axis=0)
    mini = np.min(X, axis=0)
    avg = np.mean(X, axis=0)

    # Chuẩn hóa theo công thức (X - avg) / (maxi - mini)
    X_norm = (X - avg) / (maxi - mini)

    # Thêm cột bias (cột chứa toàn số 1)
    X_b = np.c_[np.ones((N, 1)), X_norm]
    return X_b, maxi, mini, avg

from google.colab import files

# Tải file lên
uploaded = files.upload()

# Sau khi tải lên, file sẽ xuất hiện trong danh sách các file, ta có thể kiểm tra tên file:
import pandas as pd

# Giả sử file có tên là 'advertising.csv', đọc dữ liệu từ file
advertising_data = pd.read_csv('/content/advertising (1).csv')

# Kiểm tra một vài dòng đầu tiên để đảm bảo dữ liệu được tải đúng
advertising_data.head()

# Hàm SGD
def stochastic_gradient_descent(X_b, y, n_epochs=50, learning_rate=0.01):
    N = len(X_b)  # Số lượng mẫu
    thetas = np.random.randn(X_b.shape[1], 1)  # Khởi tạo trọng số ngẫu nhiên
    thetas_path = [thetas]  # Lưu lại giá trị thetas qua từng bước
    losses = []  # Lưu lại giá trị loss

    for epoch in range(n_epochs):
        for i in range(N):
            random_index = np.random.randint(N)  # Lấy mẫu ngẫu nhiên
            xi = X_b[random_index:random_index + 1]
            yi = y[random_index:random_index + 1]

            # Dự đoán y_hat
            y_hat = xi.dot(thetas)

            # Tính loss (Mean Squared Error)
            loss = (y_hat - yi) ** 2 / 2
            losses.append(loss[0][0])

            # Tính gradient
            gradients = xi.T.dot(y_hat - yi)

            # Cập nhật thetas
            thetas = thetas - learning_rate * gradients

            thetas_path.append(thetas)

    return thetas_path, losses

# Hàm Mini-batch Gradient Descent
def mini_batch_gradient_descent(X_b, y, n_epochs=50, minibatch_size=20, learning_rate=0.01):
    N = len(X_b)
    thetas = np.random.randn(X_b.shape[1], 1)
    thetas_path = [thetas]
    losses = []

    for epoch in range(n_epochs):
        shuffled_indices = np.random.permutation(N)  # Xáo trộn các chỉ số
        X_b_shuffled = X_b[shuffled_indices]
        y_shuffled = y[shuffled_indices]

        for i in range(0, N, minibatch_size):
            xi = X_b_shuffled[i:i+minibatch_size]
            yi = y_shuffled[i:i+minibatch_size]

            # Dự đoán y_hat
            y_hat = xi.dot(thetas)

            # Tính loss
            loss = (y_hat - yi) ** 2 / 2
            losses.append(np.mean(loss))

            # Tính gradient
            gradients = xi.T.dot(y_hat - yi) / len(xi)

            # Cập nhật thetas
            thetas = thetas - learning_rate * gradients

            thetas_path.append(thetas)

    return thetas_path, losses

# Hàm Batch Gradient Descent
def batch_gradient_descent(X_b, y, n_epochs=100, learning_rate=0.01):
    N = len(X_b)
    thetas = np.random.randn(X_b.shape[1], 1)
    thetas_path = [thetas]
    losses = []

    for epoch in range(n_epochs):
        # Dự đoán y_hat
        y_hat = X_b.dot(thetas)

        # Tính loss
        loss = (y_hat - y) ** 2 / 2
        losses.append(np.mean(loss))

        # Tính gradient
        gradients = X_b.T.dot(y_hat - y) / N

        # Cập nhật thetas
        thetas = thetas - learning_rate * gradients

        thetas_path.append(thetas)

    return thetas_path, losses

# Hàm Batch Gradient Descent
def batch_gradient_descent(X_b, y, n_epochs=100, learning_rate=0.01):
    N = len(X_b)
    thetas = np.random.randn(X_b.shape[1], 1)
    thetas_path = [thetas]
    losses = []

    for epoch in range(n_epochs):
        # Dự đoán y_hat
        y_hat = X_b.dot(thetas)

        # Tính loss
        loss = (y_hat - y) ** 2 / 2
        losses.append(np.mean(loss))

        # Tính gradient
        gradients = X_b.T.dot(y_hat - y) / N

        # Cập nhật thetas
        thetas = thetas - learning_rate * gradients

        thetas_path.append(thetas)

    return thetas_path, losses

import matplotlib.pyplot as plt

# Vẽ biểu đồ loss cho cả 3 phương pháp
def plot_losses(sgd_losses, mbgd_losses, bgd_losses):
    # Stochastic Gradient Descent
    plt.plot(sgd_losses[:200], label='SGD', color='red')

    # Mini-batch Gradient Descent
    plt.plot(mbgd_losses[:200], label='Mini-batch', color='blue')

    # Batch Gradient Descent
    plt.plot(bgd_losses[:100], label='Batch', color='green')

    plt.xlabel('Iterations')
    plt.ylabel('Loss')
    plt.legend()
    plt.title('Loss over Iterations for Different Gradient Descent Methods')
    plt.show()

# Normalize data
X_b, maxi, mini, avg = mean_normalization(advertising_data[['TV', 'Radio', 'Newspaper']].values)
sales_Y = advertising_data['Sales'].values.reshape(-1, 1)

# Run SGD
sgd_thetas, sgd_losses = stochastic_gradient_descent(X_b, sales_Y, n_epochs=50, learning_rate=0.01)

# Run MBGD
mbgd_thetas, mbgd_losses = mini_batch_gradient_descent(X_b, sales_Y, n_epochs=50, minibatch_size=20, learning_rate=0.01)

# Run BGD
bgd_thetas, bgd_losses = batch_gradient_descent(X_b, sales_Y, n_epochs=100, learning_rate=0.01)

# Plot losses
plot_losses(sgd_losses, mbgd_losses, bgd_losses)

