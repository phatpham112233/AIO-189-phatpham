# -*- coding: utf-8 -*-
"""Final Project Heart Disease Prediction .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EeH72u2csfR07OIoX8o94q3ApvBvy2iE
"""

#bai tap 1#
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load dataset
df = pd.read_csv('/content/cleveland.csv', header=None)
df.columns = ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target']

# Simplify target labels: 0 = No disease, 1 = Disease (1, 2, 3, 4)
df['target'] = df['target'].map({0: 0, 1: 1, 2: 1, 3: 1, 4: 1})

# Plot distribution of age vs heart disease (target)
sns.displot(df, x='age', hue='target', kde=True)
plt.title("Distribution of Age vs Heart Disease")
plt.show()

#bai tap 2#
# Barplot of age vs sex with target as hue (Heart Disease)
sns.barplot(x='age', y='sex', hue='target', data=df)
plt.title("Age, Sex vs Heart Disease")
plt.show()

#bai Tap 3#
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
from sklearn.impute import SimpleImputer # Import the SimpleImputer class

# Split data
X = df.iloc[:, :-1].values
y = df.iloc[:, -1].values
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Impute missing values with the mean
imputer = SimpleImputer(strategy='mean') # Create an imputer object with strategy='mean'
X_train = imputer.fit_transform(X_train) # Fit and transform the training data
X_test = imputer.transform(X_test) # Transform the test data

# Train KNN
knn = KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski')
knn.fit(X_train, y_train)

# Predictions
y_pred_train = knn.predict(X_train)
y_pred_test = knn.predict(X_test)

# Accuracy
accuracy_train = accuracy_score(y_train, y_pred_train)
accuracy_test = accuracy_score(y_test, y_pred_test)
print(f"Accuracy for training set for KNeighborsClassifier: {accuracy_train}")
print(f"Accuracy for test set for KNeighborsClassifier: {accuracy_test}")

#Bai tap 4#
from sklearn.svm import SVC

# Train SVM
svm = SVC(kernel='rbf', random_state=42)
svm.fit(X_train, y_train)

# Predictions
y_pred_train_svm = svm.predict(X_train)
y_pred_test_svm = svm.predict(X_test)

# Accuracy
accuracy_train_svm = accuracy_score(y_train, y_pred_train_svm)
accuracy_test_svm = accuracy_score(y_test, y_pred_test_svm)
print(f"Accuracy for training set for SVM: {accuracy_train_svm}")
print(f"Accuracy for test set for SVM: {accuracy_test_svm}")

#bai tap 5#
from sklearn.naive_bayes import GaussianNB

# Train Naive Bayes
nb = GaussianNB()
nb.fit(X_train, y_train)

# Predictions
y_pred_train_nb = nb.predict(X_train)
y_pred_test_nb = nb.predict(X_test)

# Accuracy
accuracy_train_nb = accuracy_score(y_train, y_pred_train_nb)
accuracy_test_nb = accuracy_score(y_test, y_pred_test_nb)
print(f"Accuracy for training set for Naive Bayes: {accuracy_train_nb}")
print(f"Accuracy for test set for Naive Bayes: {accuracy_test_nb}")

#bai tap 6#
from sklearn.tree import DecisionTreeClassifier

# Train Decision Tree
dt = DecisionTreeClassifier(criterion='gini', max_depth=10, min_samples_split=2, random_state=42)
dt.fit(X_train, y_train)

# Predictions
y_pred_train_dt = dt.predict(X_train)
y_pred_test_dt = dt.predict(X_test)

# Accuracy
accuracy_train_dt = accuracy_score(y_train, y_pred_train_dt)
accuracy_test_dt = accuracy_score(y_test, y_pred_test_dt)
print(f"Accuracy for training set for Decision Tree: {accuracy_train_dt}")
print(f"Accuracy for test set for Decision Tree: {accuracy_test_dt}")

#bai tap 7#
from sklearn.ensemble import RandomForestClassifier

# Train Random Forest
rf = RandomForestClassifier(criterion='gini', max_depth=10, min_samples_split=2, n_estimators=10, random_state=42)
rf.fit(X_train, y_train)

# Predictions
y_pred_train_rf = rf.predict(X_train)
y_pred_test_rf = rf.predict(X_test)

# Accuracy
accuracy_train_rf = accuracy_score(y_train, y_pred_train_rf)
accuracy_test_rf = accuracy_score(y_test, y_pred_test_rf)
print(f"Accuracy for training set for Random Forest: {accuracy_train_rf}")
print(f"Accuracy for test set for Random Forest: {accuracy_test_rf}")

#bai tap 8#
from sklearn.ensemble import AdaBoostClassifier

# Train AdaBoost
ada = AdaBoostClassifier(n_estimators=50, learning_rate=1.0, random_state=42)
ada.fit(X_train, y_train)

# Predictions
y_pred_train_ada = ada.predict(X_train)
y_pred_test_ada = ada.predict(X_test)

# Accuracy
accuracy_train_ada = accuracy_score(y_train, y_pred_train_ada)
accuracy_test_ada = accuracy_score(y_test, y_pred_test_ada)
print(f"Accuracy for training set for AdaBoost: {accuracy_train_ada}")
print(f"Accuracy for test set for AdaBoost: {accuracy_test_ada}")

#bai tap 9#
from sklearn.ensemble import GradientBoostingClassifier

# Train GradientBoost
gb = GradientBoostingClassifier(learning_rate=0.1, n_estimators=100, subsample=1.0, min_samples_split=2, max_depth=3, random_state=42)
gb.fit(X_train, y_train)

# Predictions
y_pred_train_gb = gb.predict(X_train)
y_pred_test_gb = gb.predict(X_test)

# Accuracy
accuracy_train_gb = accuracy_score(y_train, y_pred_train_gb)
accuracy_test_gb = accuracy_score(y_test, y_pred_test_gb)
print(f"Accuracy for training set for GradientBoost: {accuracy_train_gb}")
print(f"Accuracy for test set for GradientBoost: {accuracy_test_gb}")

#bai tap 10#
# Import necessary libraries
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Prepare the dataset
X = df.iloc[:, :-1].values
y = df.iloc[:, -1].values

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train the XGBoost model
xgb = XGBClassifier(objective="binary:logistic", random_state=42, n_estimators=100)
xgb.fit(X_train, y_train)

# Make predictions on training and testing data
y_pred_train_xgb = xgb.predict(X_train)
y_pred_test_xgb = xgb.predict(X_test)

# Calculate the accuracy
accuracy_train_xgb = accuracy_score(y_train, y_pred_train_xgb)
accuracy_test_xgb = accuracy_score(y_test, y_pred_test_xgb)

# Print the accuracy for both training and testing sets
print(f"Accuracy for training set for XGBoost: {accuracy_train_xgb}")
print(f"Accuracy for test set for XGBoost: {accuracy_test_xgb}")

#bai tap 11#
# Import necessary libraries
from sklearn.ensemble import StackingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.impute import SimpleImputer # Import SimpleImputer to handle NaN values

# Prepare the dataset
X = df.iloc[:, :-1].values
y = df.iloc[:, -1].values

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Impute NaN values using SimpleImputer
imputer = SimpleImputer(strategy='mean') # Use mean imputation
X_train = imputer.fit_transform(X_train)
X_test = imputer.transform(X_test)

# Define base models for stacking
estimators = [
    ('decision_tree', DecisionTreeClassifier(random_state=42)),
    ('random_forest', RandomForestClassifier(random_state=42)),
    ('knn', KNeighborsClassifier()),
    ('xgb', XGBClassifier(random_state=42)),
    ('gradient_boost', GradientBoostingClassifier(random_state=42)),
    ('svc', SVC(kernel='rbf', random_state=42)),
    ('ada_boost', AdaBoostClassifier(random_state=42))
]

# Define the stacking classifier
stacking_clf = StackingClassifier(estimators=estimators, final_estimator=RandomForestClassifier(random_state=42))

# Train the stacking classifier
stacking_clf.fit(X_train, y_train)

# Make predictions on training and testing data
y_pred_train_stack = stacking_clf.predict(X_train)
y_pred_test_stack = stacking_clf.predict(X_test)

# Calculate the accuracy
accuracy_train_stack = accuracy_score(y_train, y_pred_train_stack)
accuracy_test_stack = accuracy_score(y_test, y_pred_test_stack)

# Print the accuracy for both training and testing sets
print(f"Accuracy for training set for Stacking: {accuracy_train_stack}")
print(f"Accuracy for test set for Stacking: {accuracy_test_stack}")