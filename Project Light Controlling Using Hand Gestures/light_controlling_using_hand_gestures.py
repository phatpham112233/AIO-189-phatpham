# -*- coding: utf-8 -*-
"""Light Controlling Using Hand Gestures.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GMKYCw0J6I2_Z67r83nhR79T6XZn5mdB
"""

##Light Controlling Using Hand Gestures

!pip install mediapipe torch torchvision numpy opencv-python pyyaml

from google.colab import drive
drive.mount('/content/drive')

import yaml

gestures = {
    "gestures": {
        0: "turn_off",
        1: "light1",
        2: "light2",
        3: "light3",
        4: "turn_on"
    }
}

with open("hand_gesture.yaml", "w") as file:
    yaml.dump(gestures, file)



import cv2
import mediapipe as mp
import numpy as np
import csv
import yaml

# Load gesture labels
with open("hand_gesture.yaml", "r") as f:
    LABELS = yaml.full_load(f)["gestures"]

# Initialize MediaPipe
mp_hands = mp.solutions.hands
hands = mp_hands.Hands(static_image_mode=False, max_num_hands=1, min_detection_confidence=0.5)
mp_draw = mp.solutions.drawing_utils

# CSV file for saving data
csv_file = open("gesture_data.csv", "w", newline="")
csv_writer = csv.writer(csv_file)
csv_writer.writerow(["label"] + [f"landmark_{i}" for i in range(63)])  # 21 points * 3 (x, y, z)

# Open webcam
cap = cv2.VideoCapture(0)

current_label = None
print("Press 'a' to 'z' to assign a gesture class. Press 'q' to quit.")

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    frame = cv2.flip(frame, 1)
    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    result = hands.process(rgb_frame)

    if result.multi_hand_landmarks:
        for hand_landmarks in result.multi_hand_landmarks:
            mp_draw.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)
            landmarks = []
            for lm in hand_landmarks.landmark:
                landmarks.extend([lm.x, lm.y, lm.z])

            if current_label is not None:
                csv_writer.writerow([current_label] + landmarks)
                cv2.putText(frame, f"Recording gesture: {LABELS[current_label]}", (10, 50),
                            cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)

    cv2.imshow("Gesture Capture", frame)
    key = cv2.waitKey(1) & 0xFF

    # Assign label for the gesture
    if ord('a') <= key <= ord('z'):
        current_label = key - ord('a')

    # Quit
    if key == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()
csv_file.close()
print("Data collection complete. File saved as gesture_data.csv.")

import pandas as pd
from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader, TensorDataset
import torch

# Load data
data = pd.read_csv("gesture_data.csv")
X = data.iloc[:, 1:].values  # Landmark data
y = data.iloc[:, 0].values   # Labels

# Split data into train/test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Convert to PyTorch tensors
X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train, dtype=torch.long)
X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
y_test_tensor = torch.tensor(y_test, dtype=torch.long)

train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
test_dataset = TensorDataset(X_test_tensor, y_test_tensor)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

import torch.nn as nn

class GestureMLP(nn.Module):
    def __init__(self, input_size, num_classes):
        super(GestureMLP, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(input_size, 128),
            nn.ReLU(),
            nn.BatchNorm1d(128),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Dropout(0.4),
            nn.Linear(128, num_classes)
        )

    def forward(self, x):
        return self.model(x)

model = GestureMLP(input_size=63, num_classes=len(LABELS))

import torch.optim as optim

optimizer = optim.Adam(model.parameters(), lr=0.001)
criterion = nn.CrossEntropyLoss()

for epoch in range(10):
    model.train()
    for X_batch, y_batch in train_loader:
        optimizer.zero_grad()
        predictions = model(X_batch)
        loss = criterion(predictions, y_batch)
        loss.backward()
        optimizer.step()
    print(f"Epoch {epoch + 1}, Loss: {loss.item()}")

torch.save(model.state_dict(), "gesture_model.pth")

model.load_state_dict(torch.load("gesture_model.pth"))
model.eval()

cap = cv2.VideoCapture(0)

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    frame = cv2.flip(frame, 1)
    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    result = hands.process(rgb_frame)

    if result.multi_hand_landmarks:
        for hand_landmarks in result.multi_hand_landmarks:
            landmarks = []
            for lm in hand_landmarks.landmark:
                landmarks.extend([lm.x, lm.y, lm.z])

            input_tensor = torch.tensor([landmarks], dtype=torch.float32)
            prediction = torch.argmax(model(input_tensor), dim=1).item()
            gesture_name = LABELS[prediction]

            cv2.putText(frame, f"Gesture: {gesture_name}", (10, 50),
                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)

    cv2.imshow("Real-Time Recognition", frame)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()